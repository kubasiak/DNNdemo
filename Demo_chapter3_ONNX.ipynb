{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "check version"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "import os\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder=os.getcwd()\n",
    "data_folder='trainDataset'\n",
    "script_folder = os.path.join(os.getcwd(), \"scripts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace\n",
    "\n",
    "Create a workspace object from the existing workspace. `Workspace.from_config()` reads the file **config.json** and loads the details into an object named `ws`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "load workspace"
    ]
   },
   "outputs": [],
   "source": [
    "# load workspace configuration from the config.json file in the current folder.\n",
    "ws = Workspace.from_config()\n",
    "ds = ws.get_default_datastore()\n",
    "print(ds.datastore_type, ds.account_name, ds.container_name)\n",
    "print(ws.name, ws.location, ws.resource_group, ws.location, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create experiment\n",
    "\n",
    "Create an experiment to track the runs in your workspace. A workspace can have muliple experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create experiment"
    ]
   },
   "outputs": [],
   "source": [
    "experiment_name = 'ImageClassifier'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing compute resource\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. The code below creates the compute clusters for you if they don't already exist in your workspace.\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace the code will skip the creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = \"gpucluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Trainig Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('trainDataset', \"x_train.tsv\"),\"rb\") as f:\n",
    "     x_train=pickle.load(f)\n",
    "with open(os.path.join('trainDataset', \"y_train.tsv\"),\"rb\") as f:\n",
    "     y_train=pickle.load(f)\n",
    "with open(os.path.join('trainDataset', \"x_val.tsv\"),\"rb\") as f:\n",
    "     x_val=pickle.load(f)\n",
    "with open(os.path.join('trainDataset', \"y_val.tsv\"),\"rb\") as f:\n",
    "     y_val=pickle.load(f)\n",
    "with open(os.path.join('testDataset', \"x_test.tsv\"),\"rb\") as f:\n",
    "     x_test=pickle.load(f)\n",
    "with open(os.path.join('testDataset', \"y_test.tsv\"),\"rb\") as f:\n",
    "     y_test=pickle.load(f)\n",
    "with open(os.path.join('trainDataset', \"encoder\"),\"rb\") as f:\n",
    "     encoder=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[1], cmap=plt.cm.Greys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.upload(src_dir=data_folder, target_path='trainDataset', overwrite=True, show_progress=False)\n",
    "ds.upload(src_dir=data_folder, target_path='testDataset', overwrite=True, show_progress=False)\n",
    "print('ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain=len(x_train)\n",
    "nval=len(x_val)\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "use datastore"
    ]
   },
   "outputs": [],
   "source": [
    "%%writefile $script_folder/train_onnx.py\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from azureml.core.run import Run\n",
    "from keras import layers, models, optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "##################################################################################    <-------     ONNX\n",
    "import onnx                                                                          #<-------     ONNX\n",
    "import winmltools                                                                    #<-------     ONNX\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "parser.add_argument('--batch_size', type=str, dest='batch_size', help='batch_size')\n",
    "\n",
    "#parser.add_argument('--output', type=float, dest='output', default=0.01, help='regularization rate')\n",
    "args = parser.parse_args()\n",
    "\n",
    "data_folder = args.data_folder\n",
    "print('Data folder:', data_folder)\n",
    "batch_size = int(args.batch_size)\n",
    "print('Batch size:', batch_size)\n",
    "\n",
    "\n",
    "with open(os.path.join(data_folder, 'x_train.tsv'),\"rb\") as f:\n",
    "    x_train = pickle.load(f)\n",
    "with open(os.path.join(data_folder,'y_train.tsv'),\"rb\") as f:\n",
    "    y_train = pickle.load(f)\n",
    "with open(os.path.join(data_folder,'x_val.tsv'),\"rb\") as f:\n",
    "    x_val = pickle.load(f)\n",
    "with open(os.path.join(data_folder,'y_val.tsv'),\"rb\") as f:\n",
    "    y_val = pickle.load(f)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, sep = '\\n')\n",
    "\n",
    "ntrain=len(x_train)\n",
    "nval=len(x_val)\n",
    "\n",
    "\n",
    "###################  modeling part\n",
    "\n",
    "model=models.Sequential()\n",
    "model.add(layers.Conv2D(16,(3,3),activation='relu',input_shape=(150,150,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(32,(3,3),activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(256,activation='relu'))\n",
    "model.add(layers.Dense(8,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=1e-3), metrics=['accuracy'])\n",
    "\n",
    "trainDataGen= ImageDataGenerator(\n",
    "                    rescale=1.,\n",
    "                    rotation_range=40,\n",
    "                    width_shift_range=0.2,\n",
    "                    height_shift_range=0.2,\n",
    "                    shear_range=0.2,\n",
    "                    zoom_range=0.2\n",
    "                    )\n",
    "valDataGen=ImageDataGenerator(rescale=1.)\n",
    "\n",
    "trainGenerator=trainDataGen.flow(x_train,y_train,batch_size = batch_size)\n",
    "valGenerator=valDataGen.flow(x_val,y_val,batch_size = batch_size)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "history=model.fit_generator(trainGenerator,\n",
    "                            steps_per_epoch = ntrain//batch_size,\n",
    "                            epochs = 16,\n",
    "                            validation_data = valGenerator,\n",
    "                            validation_steps = nval//batch_size,\n",
    "                            callbacks=[early_stopping],\n",
    "                            workers=3\n",
    "                           )\n",
    "\n",
    "run.log_list('val_accuracy', history.history['val_acc'], description='Validation accuracy')\n",
    "run.log_list('val_loss', history.history['val_loss'], description='validation loss')\n",
    "\n",
    "# for now I am not using the output argument from the args\n",
    "output_dir = './outputs/model'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model_path = os.path.join(output_dir, 'onnxmodel.onnx')\n",
    "\n",
    "##################################################################################    <-------     ONNX\n",
    "TARGET_OPSET = 8 #8 for ONNX 1.3.                                                    #<------- \n",
    "convert_model = winmltools.convert_keras(model,TARGET_OPSET)                         #<-------  \n",
    "winmltools.save_model(convert_model, model_path)                                     #<-------  \n",
    "\n",
    "'''with open('./outputs/model/trainHistoryDict', 'wb') as f:\n",
    "        pickle.dump(history.history, f)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an estimator\n",
    "\n",
    "An estimator object is used to submit the run. Azure Machine Learning has pre-configured estimators for common machine learning frameworks, as well as generic Estimator. Create SKLearn estimator for scikit-learn model, by specifying\n",
    "\n",
    "* The name of the estimator object, `est`\n",
    "* The directory that contains your scripts. All the files in this directory are uploaded into the cluster nodes for execution. \n",
    "* The compute target.  In this case you will use the AmlCompute you created\n",
    "* The training script name, train.py\n",
    "* Parameters required from the training script \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "configure estimator"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ds.path('trainDataset').as_mount(),\n",
    "    '--batch_size': 16\n",
    "    \n",
    "}\n",
    "\n",
    "##################################################################################    <-------     ONNX\n",
    "est = TensorFlow(source_directory=script_folder,\n",
    "                 entry_script='train_onnx.py',\n",
    "                 script_params=script_params,\n",
    "                 compute_target=compute_target,\n",
    "                 pip_packages=['keras', 'winmltools','onnx'], #<-------     \n",
    "                 node_count=3,\n",
    "                 process_count_per_node=1,\n",
    "                 distributed_training=MpiConfiguration(),\n",
    "                 framework_version=\"1.13\",                    #<-------     \n",
    "                 use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the mounting point looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the job to the cluster\n",
    "\n",
    "Run the experiment by submitting the estimator object. And you can navigate to Azure portal to monitor the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/.amlignore\n",
    "data\n",
    "model\n",
    "sourceData\n",
    "Training\n",
    "Testing\n",
    "trainDataset\n",
    "testDataset\n",
    "dataLH\n",
    "model_keras.h5\n",
    "model_weights.h5\n",
    "model_wieghts.h5\n",
    "ARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remote run",
     "amlcompute",
     "scikit-learn"
    ]
   },
   "outputs": [],
   "source": [
    "run = experiment.submit(config=est)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor a remote run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "use notebook widget"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################    <-------     ONYX\n",
    "model_path = os.path.join('outputs','model', 'onnxmodel.onnx')\n",
    "run.download_file(model_path, output_file_path=model_path)   #############################################?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run.register_model(model_name='onnxmodel', model_path=model_path)\n",
    "print(model.name, model.id, model.version, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or any model from the workspace (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ws.models\n",
    "for name, m in models.items():\n",
    "    print(\"Name:\", name,\"\\tVersion:\", m.version, \"\\tDescription:\", m.description, m.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "import os\n",
    "\n",
    "model = Model(workspace=ws, name=\"onyxmodel\", version=10)\n",
    "model.download(target_dir=os.getcwd(),exist_ok=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/tutorials/img-classification-part1-training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from azureml.core.model import Model\n",
    "import onnxruntime as rt\n",
    "\n",
    "def init():\n",
    "    global session\n",
    "    model_path = Model.get_model_path(model_name='onnxmodel')\n",
    "    session = rt.InferenceSession(model_path)\n",
    "\n",
    "\n",
    "def run(raw_data):\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    data = data.astype(np.float32)\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    \n",
    "    # make prediction\n",
    "    res = session.run(None, {input_name: data})\n",
    "    \n",
    "    prob = res[0]\n",
    "\n",
    "    return prob.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating container image"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##### ===========================================================\n",
    "=============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "##################################################################################          <-------     ONYX\n",
    "myenv = CondaDependencies.create(pip_packages=[\"numpy\",\"onnxruntime\",\"azureml-defaults\"]   #<-------     ONYX\n",
    "                                )\n",
    "myenv.add_conda_package('tensorflow')\n",
    "myenv.add_conda_package('keras')\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(runtime=\"python\",\n",
    "                                   entry_script=\"score.py\",\n",
    "                                   conda_file=\"myenv.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import  AciWebservice\n",
    "from azureml.core.model import Model\n",
    "\n",
    "deployment_config = AciWebservice.deploy_configuration()\n",
    "service = Model.deploy(ws, \"myonnxservice\", [model], inference_config, deployment_config)\n",
    "service.wait_for_deployment(show_output = True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===========================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "=============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join('testDataset', \"x_test.tsv\"),\"rb\") as f:\n",
    "     x_test=pickle.load(f)\n",
    "with open(os.path.join('testDataset', \"y_test.tsv\"),\"rb\") as f:\n",
    "     y_test=pickle.load(f)\n",
    "with open(os.path.join('trainDataset', \"encoder\"),\"rb\") as f:\n",
    "     encoder=pickle.load(f)\n",
    "\n",
    "idx=12\n",
    "test_sample_x=x_test[idx]\n",
    "test_sample_y=y_test[idx]\n",
    "\n",
    "testx=np.expand_dims(test_sample_x, axis=0)\n",
    "test_json=json.dumps({'data':testx.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = service.run(input_data = test_json)\n",
    "result=pd.DataFrame(np.around(predicted[0],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['MaxVal']=''\n",
    "result['MaxVal'][np.argmax(predicted)]= ' *'\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predLab=np.argmax(predicted)\n",
    "predictedLabel= encoder.inverse_transform([predLab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "labelsValid=np.core.defchararray.add(np.core.defchararray.add(test_sample_y,' / '),predictedLabel[0])\n",
    "\n",
    "title_obj=plt.title(labelsValid, fontsize=12)\n",
    "if (predictedLabel[0]!=test_sample_y):\n",
    "        plt.setp(title_obj, color='r') \n",
    "plt.imshow(test_sample_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "roastala"
   }
  ],
  "categories": [
   "tutorials"
  ],
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "msauthor": "roastala"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
